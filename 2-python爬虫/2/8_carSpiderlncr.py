"""
    æ±½è½¦ä¹‹å®¶äºŒé¡µæ•°æ®æŠ“å–æ¡ˆä¾‹ - åˆ©ç”¨rediså®ç°å¢é‡çˆ¬è™«
        1.ä¸€çº§é¡µé¢æå–æ•°æ®ï¼šæ±½è½¦è¯¦æƒ…é¡µè¿æ¥
        2.äºŒçº§é¡µé¢æå–æ•°æ®ï¼šå…·ä½“æ±½è½¦æ•°æ®
        3.å»ºç«‹User-Agentæ± æ¥åº”å¯¹åçˆ¬è™«
            fake_useragentæ¨¡å—



å¢é‡çˆ¬è™«æ€è·¯ï¼š
    1,åˆ©ç”¨redisé›†åˆå­˜å‚¨æ‰€æœ‰è¯·æ±‚çš„æŒ‡çº¹
    2,åˆ©ç”¨saddï¼ˆï¼‰çš„è¿”å›å€¼æ¥åˆ¤æ–­ä¹‹å‰æ˜¯å¦æŠ“å–è¿‡æ¬¡è¯·æ±‚

"""

import random
import re
import requests
import time
import pandas as pd
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import redis
from hashlib import md5
import sys



class CarSpider:
    def __init__(self):
        self.url = 'https://www.che168.com/beijing/a0_0msdgscncgpi1ltocsp{}exx0/'
        # è¿æ¥redis
        self.r = redis.Redis(host='localhost', port=6379, db=0)

    def get_html(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        headers = {'User-Agent': UserAgent().random}
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = response.apparent_encoding
            print(f"ğŸ“Š æ£€æµ‹åˆ°çš„ç¼–ç : {response.encoding}")
            return response.text
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥: {e}")
            return ""

    def parse_html(self, one_url):
        """è§£æåˆ—è¡¨é¡µ"""
        print(f"ğŸŒ æ­£åœ¨è®¿é—®: {one_url}")
        html = self.get_html(one_url)

        if not html:
            print("âŒ é¡µé¢å†…å®¹ä¸ºç©º")
            return

        # ä¿å­˜HTMLå†…å®¹ç”¨äºè°ƒè¯•
        try:
            with open('debug_page.html', 'w', encoding='utf-8') as f:
                f.write(html)
            print("ğŸ’¾ é¡µé¢å·²ä¿å­˜ä¸º debug_page.html")
        except Exception as e:
            print(f"âŒ ä¿å­˜é¡µé¢å¤±è´¥: {e}")

        # æ˜¾ç¤ºé¡µé¢æ ‡é¢˜
        title_match = re.search(r'<title>(.*?)</title>', html, re.S)
        if title_match:
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title_match.group(1)}")

        # ä½¿ç”¨BeautifulSoupè§£æ
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # æ–¹æ³•1: æŸ¥æ‰¾cards-liç±»çš„å…ƒç´ 
            car_items = soup.find_all('li', class_='cards-li')
            print(f"ğŸœ é€šè¿‡classæ‰¾åˆ° {len(car_items)} ä¸ªè½¦è¾†é¡¹")

            car_links = []
            for item in car_items:
                link = item.find('a')
                if link and link.get('href'):
                    href = link['href']
                    car_links.append(href)
                    print(f"  æ‰¾åˆ°é“¾æ¥: {href}")

            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«/dealer/çš„é“¾æ¥
            if not car_links:
                print("ğŸ” å°è¯•æŸ¥æ‰¾/dealer/é“¾æ¥...")
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link.get('href', '')
                    if '/dealer/' in href and href.endswith('.html'):
                        car_links.append(href)
                        print(f"  æ‰¾åˆ°ç»é”€å•†é“¾æ¥: {href}")

            # æ–¹æ³•3: æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„è½¦è¾†é“¾æ¥
            if not car_links:
                print("ğŸ” å°è¯•æŸ¥æ‰¾å…¶ä»–é“¾æ¥æ¨¡å¼...")
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å¯èƒ½çš„è½¦è¾†é“¾æ¥
                link_patterns = [
                    r'href="(/dealer/.*?\.html)"',
                    r'href="(.*?\.html\?pvareaid=.*?)"',
                    r'class="carinfo".*?href="(.*?)"'
                ]

                for pattern in link_patterns:
                    matches = re.findall(pattern, html, re.S | re.I)
                    if matches:
                        car_links.extend(matches)
                        print(f"  æ­£åˆ™æ‰¾åˆ° {len(matches)} ä¸ªé“¾æ¥")
                        break

            if not car_links:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•è½¦è¾†é“¾æ¥")
                # æ˜¾ç¤ºé¡µé¢å¼€å¤´å†…å®¹ç”¨äºè°ƒè¯•
                print("HTMLå¼€å¤´500å­—ç¬¦:", html[:500])
                return

            print(f"âœ… æ€»å…±æ‰¾åˆ° {len(car_links)} ä¸ªè½¦è¾†é“¾æ¥")

            # å¤„ç†å‰3ä¸ªé“¾æ¥è¿›è¡Œæµ‹è¯•
            for j, href in enumerate(car_links[:3]):
                if href.startswith('http'):
                    two_url = href
                else:
                    two_url = 'https://www.che168.com' + href
            finger = self.md5_url(url=two_url)
            #å¦‚æœè¿”å›å€¼ä¸º1ï¼šæ·»åŠ æˆåŠŸï¼Œè¯´æ˜ä¹‹å‰æ²¡æœ‰æŠ“å–è¿‡æ¬¡åœ°å€
            if self.r.sadd('car:spiders', finger) == 1:

                print(f"ğŸš— å¤„ç†ç¬¬ {j + 1} ä¸ª: {two_url}")
                self.get_car_info(two_url)
                time.sleep(random.uniform(1, 2))

            else:
                # ä¸€æ—¦å‘ç°ç¬¬ä¸€ä¸ªå·²ç»æŠ“å–è¿‡ã€‚è¯´æ˜åé¢çš„ä¹Ÿä¸€å®šæŠ“å–è¿‡äº†ï¼Œæ­¤æ—¶ä¼šå½»åº•é€€å‡º
                sys.exit('æ›´æ–°å®Œæˆï¼Œç¨‹åºé€€å‡º')


        except Exception as e:
            print(f"âŒ è§£æHTMLå¤±è´¥: {e}")

    def md5_url(self, url):
        """åŠŸèƒ½å‡½æ•°3 - ç”ŸæˆæŒ‡çº¹çš„å‡½æ•°"""
        s = md5()
        s.update(url.encode())
        return s.hexdigest()



    def get_car_info(self, two_url):
        """æå–è½¦è¾†è¯¦æƒ…ä¿¡æ¯"""
        print(f"ğŸ“– è§£æè¯¦æƒ…é¡µ: {two_url}")
        html = self.get_html(two_url)

        if not html:
            return

        try:
            # ç®€å•æå–ä¿¡æ¯
            title_match = re.search(r'<title>(.*?)</title>', html, re.S)
            price_match = re.search(r'class="price".*?([\d\.]+ä¸‡)', html, re.S)

            title = title_match.group(1).strip() if title_match else 'æœªçŸ¥æ ‡é¢˜'
            price = price_match.group(1) if price_match else 'ä»·æ ¼æœªçŸ¥'

            car_info = {
                'è½¦å‹': title.split('|')[0] if '|' in title else title,
                'ä»·æ ¼': price,
                'é“¾æ¥': two_url,
                'çˆ¬å–æ—¶é—´': time.strftime('%Y-%m-%d %H:%M:%S')
            }

            self.car_list.append(car_info)
            print(f"âœ… æå–: {car_info['è½¦å‹']} - {car_info['ä»·æ ¼']}")

        except Exception as e:
            print(f"âŒ è§£æè¯¦æƒ…é¡µå¤±è´¥: {e}")

    def save_to_excel(self):
        """ä¿å­˜æ•°æ®åˆ°Excel - ä¿®æ­£äº†encodingå‚æ•°"""
        if self.car_list:
            df = pd.DataFrame(self.car_list)
            # ä¿®æ­£ï¼šå»æ‰ encoding å‚æ•°
            df.to_excel('äºŒæ‰‹è½¦æ•°æ®.xlsx', index=False)
            print(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(self.car_list)} æ¡æ•°æ®")
        else:
            print("âŒ æ— æ•°æ®å¯ä¿å­˜")

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸš— å¼€å§‹çˆ¬å–äºŒæ‰‹è½¦ä¹‹å®¶æ•°æ®...")

        # æµ‹è¯•ç¬¬ä¸€é¡µ
        page_url = self.url.format(1)
        self.parse_html(page_url)

        self.save_to_excel()
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    spider = CarSpider()
    spider.run()